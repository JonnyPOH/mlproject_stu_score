{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler\n",
    "\n",
    "from src.exception import CustomException\n",
    "from src.logger import logging\n",
    "import os\n",
    "\n",
    "from src.utils import save_object\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    preprocessor_ob_file_path = os.path.join('artifacts', 'preprocessor.pkl')\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self):\n",
    "        self.data_transformation_config = DataTransformationConfig()\n",
    "\n",
    "    def get_transformer_object(self):\n",
    "        try:\n",
    "            # Define numerical and categorical columns\n",
    "            numerical_columns = ['AGE', 'INCOME']\n",
    "            categorical_columns = ['GENDER', 'VETERAN', 'substanceabuse', 'completed', 'probation', 'assistancetype', 'required']\n",
    "\n",
    "            # Pipeline for numerical columns\n",
    "            num_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                    (\"scaler\", MaxAbsScaler())\n",
    "                ])\n",
    "\n",
    "            # Pipeline for categorical columns\n",
    "            cat_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            logging.info(\"Column transformer initiated\")\n",
    "\n",
    "            # Combine numerical and categorical pipelines\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"num\", num_pipeline, numerical_columns),\n",
    "                    (\"cat\", cat_pipeline, categorical_columns)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            return preprocessor\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def initiate_data_transformation(self, train_path, test_path, columns_to_drop=None):\n",
    "        try:\n",
    "            # Read train and test datasets\n",
    "            train_df = pd.read_csv(train_path)\n",
    "            test_df = pd.read_csv(test_path)\n",
    "\n",
    "            logging.info(\"Read train and test data successfully\")\n",
    "\n",
    "            # Drop specified columns if 'columns_to_drop' is provided\n",
    "            if columns_to_drop:\n",
    "                logging.info(f\"Dropping columns: {columns_to_drop}\")\n",
    "                train_df = train_df.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "                test_df = test_df.drop(columns=columns_to_drop, axis=1, errors='ignore')\n",
    "\n",
    "            logging.info(f\"Applying preprocessor to train and test data\")\n",
    "\n",
    "            # Get the preprocessor\n",
    "            preprocessing_obj = self.get_transformer_object()\n",
    "\n",
    "            target_column_name = 'NIGHTS'\n",
    "            numerical_columns = ['AGE', 'INCOME']\n",
    "\n",
    "            # Separate features and target for train and test datasets\n",
    "            input_feature_train_df = train_df.drop(columns=[target_column_name], axis=1)\n",
    "            target_feature_train_df = train_df[target_column_name]\n",
    "\n",
    "            input_feature_test_df = test_df.drop(columns=[target_column_name], axis=1)\n",
    "            target_feature_test_df = test_df[target_column_name]\n",
    "\n",
    "            # Apply preprocessing to train and test data\n",
    "            input_feature_train_arr = preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            input_feature_test_arr = preprocessing_obj.transform(input_feature_test_df)\n",
    "\n",
    "            # Save the preprocessor object to file\n",
    "            save_object(\n",
    "                file_path=self.data_transformation_config.preprocessor_ob_file_path,\n",
    "                obj=preprocessing_obj\n",
    "            )\n",
    "\n",
    "            logging.info(\"Data transformation completed successfully\")\n",
    "\n",
    "            return (\n",
    "                # train_arr,\n",
    "                # test_arr,\n",
    "                self.data_transformation_config.preprocessor_ob_file_path\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
